{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz already exists.\n",
      "train-labels-idx1-ubyte.gz already exists.\n",
      "t10k-images-idx3-ubyte.gz already exists.\n",
      "t10k-labels-idx1-ubyte.gz already exists.\n",
      "Shape of train_images: (60000, 28, 28)\n",
      "Shape of train_labels: (60000,)\n",
      "Shape of test_images: (10000, 28, 28)\n",
      "Shape of test_labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# flake8: noqa: E302\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(url, save_path, folder_path):\n",
    "    \"\"\"Download and extract dataset if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        decompressed_file_name = os.path.splitext(os.path.basename(save_path))[0]\n",
    "        decompressed_file_path = os.path.join(folder_path, decompressed_file_name)\n",
    "\n",
    "        with gzip.open(save_path, \"rb\") as f_in:\n",
    "            with open(decompressed_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        print(f\"{decompressed_file_name} downloaded and extracted.\")\n",
    "    else:\n",
    "        print(f\"{os.path.basename(save_path)} already exists.\")\n",
    "\n",
    "\n",
    "file_info = [\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "        \"train-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "        \"train-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "        \"t10k-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
    "        \"t10k-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "folder_name = \"tmp/mnist\"\n",
    "folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Download and extract each file\n",
    "for url, file_name in file_info:\n",
    "    path_to_save = os.path.join(folder_path, file_name)\n",
    "    download_and_extract_dataset(url, path_to_save, folder_path)\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_idx3_ubyte_image_file(filename):\n",
    "    \"\"\"Read IDX3-ubyte formatted image data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_images = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2051:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        images = np.zeros((num_images, num_rows, num_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for r in range(num_rows):\n",
    "                for c in range(num_cols):\n",
    "                    pixel = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "                    images[i, r, c] = pixel\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_idx1_ubyte_label_file(filename):\n",
    "    \"\"\"Read IDX1-ubyte formatted label data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_labels = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2049:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        labels = np.zeros(num_labels, dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            labels[i] = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = os.path.join(\n",
    "    os.getcwd(), folder_name\n",
    ")  # Adjust this path to where you stored the files\n",
    "\n",
    "train_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"train-images-idx3-ubyte\")\n",
    ")\n",
    "train_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"train-labels-idx1-ubyte\")\n",
    ")\n",
    "test_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"t10k-images-idx3-ubyte\")\n",
    ")\n",
    "test_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"t10k-labels-idx1-ubyte\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shape of train_images: {train_images.shape}\"\n",
    ")  # Should output \"Shape of train_images: (60000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of train_labels: {train_labels.shape}\"\n",
    ")  # Should output \"Shape of train_labels: (60000,)\"\n",
    "print(\n",
    "    f\"Shape of test_images: {test_images.shape}\"\n",
    ")  # Should output \"Shape of test_images: (10000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of test_labels: {test_labels.shape}\"\n",
    ")  # Should output \"Shape of test_labels: (10000,)\"\n",
    "\n",
    "# %%\n",
    "# Reshape the datasets from 3D to 2D\n",
    "train_images_2d = train_images.reshape(\n",
    "    train_images.shape[0], -1\n",
    ")  # -1 infers the size from the remaining dimensions\n",
    "test_images_2d = test_images.reshape(test_images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertto pytorch tensors\n",
    "import torch\n",
    "\n",
    "train_images_tensor_initial = torch.from_numpy(train_images_2d).float()\n",
    "train_labels_tensor_initial = torch.from_numpy(train_labels).long()\n",
    "test_images_tensor = torch.from_numpy(test_images_2d).float()\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "\n",
    "# seed the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# shuffle the training dataset\n",
    "indices = torch.randperm(train_images_tensor_initial.shape[0])\n",
    "train_images_tensor_shuffled = train_images_tensor_initial[indices]\n",
    "train_labels_tensor_shuffled = train_labels_tensor_initial[indices]\n",
    "\n",
    "# get a 10% validation set\n",
    "validation_size = int(train_images_tensor_shuffled.shape[0] * 0.1)\n",
    "validation_images_tensor = train_images_tensor_shuffled[:validation_size]\n",
    "validation_labels_tensor = train_labels_tensor_shuffled[:validation_size]\n",
    "train_images_tensor = train_images_tensor_shuffled[validation_size:]\n",
    "train_labels_tensor = train_labels_tensor_shuffled[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54000])\n",
      "torch.Size([6000])\n",
      "torch.Size([60000])\n",
      "torch.Size([60000])\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_tensor.shape)\n",
    "print(validation_labels_tensor.shape)\n",
    "print(train_labels_tensor_initial.shape)\n",
    "print(train_labels_tensor_shuffled.shape)\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training labels size: 60000\n",
      "Validation set size: 6000\n",
      "Validation labels size after split: 6000\n",
      "Training labels size after split: 54000\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial training labels size:\", train_labels_tensor_initial.shape[0])\n",
    "print(\"Validation set size:\", validation_size)\n",
    "print(\"Validation labels size after split:\", validation_labels_tensor.shape[0])\n",
    "print(\"Training labels size after split:\", train_labels_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([28, 28])\n",
      "Label tensor(0)\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "\n",
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a bounding box of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_bounding_box(img):\n",
    "    \"\"\"\n",
    "    Extract the bounding box from an MNIST image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): 2D numpy array representing the MNIST image.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): Cropped image with the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert torch image to numpy array\n",
    "    img = img.numpy()\n",
    "\n",
    "    # Find the rows and columns where the image has non-zero pixels\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "\n",
    "    # Find the first and last row and column indices where the image has non-zero pixels\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return the cropped image\n",
    "    return img[rmin : rmax + 1, cmin : cmax + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (20, 14)\n",
      "Label tensor(0)\n"
     ]
    }
   ],
   "source": [
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "cropped_image = get_bounding_box(image)\n",
    "\n",
    "plt.imshow(cropped_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", cropped_image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the image quadratic (20x20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "resized_image = cv2.resize(cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (20, 20)\n",
      "Label tensor(0)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(resized_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", resized_image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a reshaped image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_images_tensor)\n",
    "\n",
    "train_images_tensor_resized = np.zeros((num_train, 400))\n",
    "\n",
    "for i in range(num_train):\n",
    "    cropped_image = get_bounding_box(train_images_tensor[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    train_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "num_test = len(test_images_tensor)\n",
    "\n",
    "num_val = len(validation_images_tensor)\n",
    "\n",
    "validation_images_tensor_resized = np.zeros((num_val, 400))\n",
    "\n",
    "for i in range(num_val):\n",
    "    cropped_image = get_bounding_box(validation_images_tensor[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    validation_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "num_test = len(test_images_tensor)\n",
    "\n",
    "test_images_tensor_resized = np.zeros((num_test, 400))\n",
    "\n",
    "for i in range(num_test):\n",
    "    cropped_image = get_bounding_box(test_images_tensor[i].reshape(28, 28))\n",
    "    cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "    resized_image = cv2.resize(\n",
    "        cropped_image_uint8, (20, 20), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    test_images_tensor_resized[i, :] = resized_image.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = torch.tensor(scaler.fit_transform(train_images_tensor))\n",
    "val_features_normalized = torch.tensor(scaler.transform(validation_images_tensor))\n",
    "test_features_normalized = torch.tensor(scaler.transform(test_images_tensor))\n",
    "\n",
    "train_features_resized_normalized = torch.tensor(\n",
    "    scaler.fit_transform(train_images_tensor_resized)\n",
    ")\n",
    "val_features_resized_normalized = torch.tensor(\n",
    "    scaler.transform(validation_images_tensor_resized)\n",
    ")\n",
    "test_features_resized_normalized = torch.tensor(\n",
    "    scaler.transform(test_images_tensor_resized)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = train_features_normalized.float()\n",
    "val_features_normalized = val_features_normalized.float()\n",
    "test_features_normalized = test_features_normalized.float()\n",
    "\n",
    "train_features_resized_normalized = train_features_resized_normalized.float()\n",
    "val_features_resized_normalized = val_features_resized_normalized.float()\n",
    "test_features_resized_normalized = test_features_resized_normalized.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a medium-sized neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define code to measure accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into haar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_haar_features(image):\n",
    "    if image.shape != (20, 20) and image.shape != (28, 28):\n",
    "        raise ValueError(\"Input image must be of shape 20x20 or 28x28.\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Sliding window\n",
    "    for i in range(0, image.shape[0], 3):  # Slide vertically with a step of 3\n",
    "        for j in range(0, image.shape[0], 3):  # Slide horizontally with a step of 3\n",
    "\n",
    "            if i + 6 > image.shape[0] or j + 6 > image.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Extract 6x6 window\n",
    "            window = image[i : i + 6, j : j + 6]\n",
    "\n",
    "            # Horizontal feature\n",
    "            horizontal_feature_value = np.sum(window[0:3, :]) - np.sum(window[3:6, :])\n",
    "\n",
    "            # Vertical feature\n",
    "            vertical_feature_value = np.sum(window[:, 0:3]) - np.sum(window[:, 3:6])\n",
    "\n",
    "            features.append(horizontal_feature_value)\n",
    "            features.append(vertical_feature_value)\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.     0.     0.     0.     0.     0.  -117.  -117. -1480. -1246.\n",
      " -1363.  1363.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -18.   -18. -1409. -1607. -2169. -1879. -1502.  2780.  -724.   724.\n",
      "     0.     0.     0.     0.     0.     0.  -847.  -883. -1244. -2530.\n",
      "  1435.   963.   501.  -329. -1384.  2726.   -53.    53.     0.     0.\n",
      "   -41.   -41. -1010. -2658.   508.   366.  1663.  1901.   -36. -3900.\n",
      "  -331.  4117.  -109.   215.     0.     0.  -488.  -570.  -253. -2863.\n",
      "   663.  3005.  -386.  -632.  -314. -2994.   643.  3873.   143.   181.\n",
      "     0.     0.   -47. -1105.   224. -1822.  -145.  2511. -1552. -2594.\n",
      "   -30.   562.  1125.  2429.    19.    19.     0.     0.   281.  -871.\n",
      "  -205. -2271. -1857.   939.    35.  -537.  2077.  2069.   671.   671.\n",
      "     0.     0.     0.     0.   295.  -295.  2109. -1519.  3601.    27.\n",
      "  2454.  1120.   667.   667.     0.     0.     0.     0.]\n",
      "length of a haar feature for 28x28 images 128\n"
     ]
    }
   ],
   "source": [
    "haar_1 = compute_haar_features(train_images_tensor[0].reshape(28, 28).numpy())\n",
    "print(haar_1)\n",
    "len_haar_features = len(haar_1)\n",
    "print(\"length of a haar feature for 28x28 images\", len_haar_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -339.  -339. -1844. -1810. -1511. -1207.  -131.  -249. -1376.  2298.\n",
      " -1690. -2094. -1410. -1280.  1610.  1616.  2663.  -639.   271.  -949.\n",
      " -1000. -2236.  1667.  1809.  1742.  1622.   -53. -1387.  -292. -2832.\n",
      "    98.   608.   988.  2522.  -288.  -292. -1582. -2562.  -412.  -726.\n",
      "  -110.  2028. -1076.  1026. -2496. -1492. -1082. -1242.  1760.  1966.]\n",
      "length of a haar feature for 20x20 images 50\n"
     ]
    }
   ],
   "source": [
    "haar_1 = compute_haar_features(train_images_tensor_resized[0].reshape(20, 20))\n",
    "print(haar_1)\n",
    "len_haar_features_resized = len(haar_1)\n",
    "print(\"length of a haar feature for 20x20 images\", len_haar_features_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_ratio(image, threshold=0.5):\n",
    "    # Threshold the image to create a binary representation\n",
    "    bin_image = image > threshold\n",
    "    # Find the bounding box\n",
    "    row_indices, col_indices = np.nonzero(bin_image)\n",
    "    max_row, min_row = np.max(row_indices), np.min(row_indices)\n",
    "    max_col, min_col = np.max(col_indices), np.min(col_indices)\n",
    "\n",
    "    # Calculate the aspect ratio of the bounding box\n",
    "    width = max_col - min_col + 1\n",
    "    height = max_row - min_row + 1\n",
    "\n",
    "    if height == 0:  # To avoid division by zero\n",
    "        return 1.0\n",
    "\n",
    "    return width / height\n",
    "\n",
    "\n",
    "from scipy.ndimage import label as nd_label\n",
    "\n",
    "\n",
    "def num_regions_below_threshold(image, threshold=0.5):\n",
    "    # Threshold the image so that pixels below the threshold are set to 1\n",
    "    # and those above the threshold are set to 0.\n",
    "    bin_image = image < threshold\n",
    "\n",
    "    # Use connected components labeling\n",
    "    labeled_array, num_features = nd_label(bin_image)\n",
    "\n",
    "    # Return the number of unique regions\n",
    "    # (subtracting 1 as one of the labels will be the background)\n",
    "    return num_features\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "import numpy as np\n",
    "\n",
    "def eccentricity(image, threshold=0.5):\n",
    "    bin_image = image > threshold\n",
    "    labeled_img = label(bin_image)\n",
    "    properties = regionprops(labeled_img)\n",
    "    if properties:\n",
    "        return properties[0].eccentricity\n",
    "    return 0\n",
    "\n",
    "def extent(image, threshold=0.5):\n",
    "    bin_image = image > threshold\n",
    "    object_area = np.sum(bin_image)\n",
    "    _, _, width, height = cv2.boundingRect(bin_image.astype(np.uint8))\n",
    "    bounding_box_area = width * height\n",
    "    return object_area / bounding_box_area if bounding_box_area != 0 else 0\n",
    "\n",
    "import cv2\n",
    "\n",
    "def edge_density(image):\n",
    "    edges = cv2.Canny(image.astype(np.uint8), 100, 200)\n",
    "    return np.sum(edges) / (image.shape[0] * image.shape[1])\n",
    "\n",
    "def circularity(image, threshold=0.5):\n",
    "    bin_image = image > threshold\n",
    "    contours, _ = cv2.findContours(bin_image.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    perimeter = cv2.arcLength(contours[0], True)\n",
    "    area = np.sum(bin_image)\n",
    "    if area == 0:\n",
    "        return 0\n",
    "    return (perimeter**2) / area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.morphology import skeletonize\n",
    "from scipy.stats import skew\n",
    "\n",
    "def bounding_box_ratio(image):\n",
    "    # Find the bounding box\n",
    "    y, x = np.where(image == 0)  # Assuming the digit is black (0) on a white (255) background\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    y_min, y_max = np.min(y), np.max(y)\n",
    "    \n",
    "    bb_width = x_max - x_min\n",
    "    bb_height = y_max - y_min\n",
    "    \n",
    "    image_height, image_width = image.shape\n",
    "    \n",
    "    return (bb_width / image_width, bb_height / image_height)\n",
    "\n",
    "def distance_from_center(image):\n",
    "    y, x = np.where(image == 0)\n",
    "    center_y, center_x = image.shape[0] // 2, image.shape[1] // 2\n",
    "    distances = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)\n",
    "    return np.mean(distances)\n",
    "\n",
    "def radial_moments(image):\n",
    "    y, x = np.where(image == 0)\n",
    "    center_y, center_x = image.shape[0] // 2, image.shape[1] // 2\n",
    "    distances = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)\n",
    "    return np.mean(distances), np.var(distances), skew(distances)\n",
    "\n",
    "def convexity(image):\n",
    "    # Convert the image to 8-bit unsigned integer format (if it's not already)\n",
    "    image_8bit = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(image_8bit, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    hull = cv2.convexHull(contour)\n",
    "    contour_area = cv2.contourArea(contour)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    return contour_area / hull_area\n",
    "\n",
    "\n",
    "def skeleton_features(image):\n",
    "    skeleton = skeletonize(image == 0)  # Assumes the digit is black on white background\n",
    "    cross_kernel = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "    endpoints_kernel = np.array([[0, 0, 0], [0, 1, 0], [2, 1, 2]])\n",
    "    \n",
    "    crossings = cv2.filter2D(skeleton.astype(np.uint8), -1, cross_kernel)\n",
    "    endpoints = cv2.filter2D(skeleton.astype(np.uint8), -1, endpoints_kernel)\n",
    "    \n",
    "    num_crossings = (crossings == 3).sum()\n",
    "    num_endpoints = (endpoints == 1).sum()\n",
    "    \n",
    "    return num_endpoints, num_crossings\n",
    "\n",
    "def crossing_counts(image):\n",
    "    rows_crossings = np.sum(np.abs(np.diff(image, axis=1)) == 255, axis=1)\n",
    "    cols_crossings = np.sum(np.abs(np.diff(image, axis=0)) == 255, axis=0)\n",
    "    return rows_crossings, cols_crossings\n",
    "\n",
    "def pixel_count(image):\n",
    "    return (image == 0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "aspect_ratio_train = np.zeros(num_train)\n",
    "aspect_ratio_val = np.zeros(num_val)\n",
    "aspect_ratio_test = np.zeros(num_test)\n",
    "\n",
    "num_white_regions_train = np.zeros(num_train)\n",
    "num_white_regions_val = np.zeros(num_val)\n",
    "num_white_regions_test = np.zeros(num_test)\n",
    "\n",
    "eccentricity_train = np.zeros(num_train)\n",
    "eccentricity_val = np.zeros(num_val)\n",
    "eccentricity_test = np.zeros(num_test)\n",
    "\n",
    "extent_train = np.zeros(num_train)\n",
    "extent_val = np.zeros(num_val)\n",
    "extent_test = np.zeros(num_test)\n",
    "\n",
    "edge_density_train = np.zeros(num_train)\n",
    "edge_density_val = np.zeros(num_val)\n",
    "edge_density_test = np.zeros(num_test)\n",
    "\n",
    "circularity_train = np.zeros(num_train)\n",
    "circularity_val = np.zeros(num_val)\n",
    "circularity_test = np.zeros(num_test)\n",
    "\n",
    "for i in range(num_train):\n",
    "    aspect_ratio_train[i] = aspect_ratio(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_train[i] = num_regions_below_threshold(\n",
    "        train_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    eccentricity_train[i] = eccentricity(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    extent_train[i] = extent(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    edge_density_train[i] = edge_density(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    circularity_train[i] = circularity(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "\n",
    "for i in range(num_val):\n",
    "    aspect_ratio_val[i] = aspect_ratio(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    num_white_regions_val[i] = num_regions_below_threshold(\n",
    "        validation_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    eccentricity_val[i] = eccentricity(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    extent_val[i] = extent(validation_images_tensor[i].reshape(28, 28).numpy())\n",
    "    edge_density_val[i] = edge_density(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    aspect_ratio_test[i] = aspect_ratio(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_test[i] = num_regions_below_threshold(\n",
    "        test_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    eccentricity_test[i] = eccentricity(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    extent_test[i] = extent(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    edge_density_test[i] = edge_density(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    circularity_test[i] = circularity(test_images_tensor[i].reshape(28, 28).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding Box Ratio\n",
    "bounding_box_ratio_train = np.zeros(num_train)\n",
    "bounding_box_ratio_val = np.zeros(num_val)\n",
    "bounding_box_ratio_test = np.zeros(num_test)\n",
    "\n",
    "# Distance from Center\n",
    "distance_from_center_train = np.zeros(num_train)\n",
    "distance_from_center_val = np.zeros(num_val)\n",
    "distance_from_center_test = np.zeros(num_test)\n",
    "\n",
    "# Radial Moments\n",
    "radial_moments_train = np.zeros((num_train, 3))  # mean, variance, skewness\n",
    "radial_moments_val = np.zeros((num_val, 3))\n",
    "radial_moments_test = np.zeros((num_test, 3))\n",
    "\n",
    "# Convexity\n",
    "convexity_train = np.zeros(num_train)\n",
    "convexity_val = np.zeros(num_val)\n",
    "convexity_test = np.zeros(num_test)\n",
    "\n",
    "# Skeleton features\n",
    "skeleton_endpoints_train = np.zeros(num_train)\n",
    "skeleton_endpoints_val = np.zeros(num_val)\n",
    "skeleton_endpoints_test = np.zeros(num_test)\n",
    "\n",
    "skeleton_crossings_train = np.zeros(num_train)\n",
    "skeleton_crossings_val = np.zeros(num_val)\n",
    "skeleton_crossings_test = np.zeros(num_test)\n",
    "\n",
    "# Crossing Counts\n",
    "# Assuming the image is square, else adjust accordingly\n",
    "crossing_counts_rows_train = np.zeros((num_train, 28))\n",
    "crossing_counts_rows_val = np.zeros((num_val, 28))\n",
    "crossing_counts_rows_test = np.zeros((num_test, 28))\n",
    "\n",
    "crossing_counts_cols_train = np.zeros((num_train, 28))\n",
    "crossing_counts_cols_val = np.zeros((num_val, 28))\n",
    "crossing_counts_cols_test = np.zeros((num_test, 28))\n",
    "\n",
    "# Pixel Count\n",
    "pixel_count_train = np.zeros(num_train)\n",
    "pixel_count_val = np.zeros(num_val)\n",
    "pixel_count_test = np.zeros(num_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train):\n",
    "    image = train_images_tensor[i].reshape(28, 28).numpy()\n",
    "    bounding_box_ratio_train[i] = np.mean(bounding_box_ratio(image))  # Taking average of width and height ratios\n",
    "    distance_from_center_train[i] = distance_from_center(image)\n",
    "    radial_moments_train[i] = radial_moments(image)\n",
    "    convexity_train[i] = convexity(image)\n",
    "    skeleton_endpoints_train[i], skeleton_crossings_train[i] = skeleton_features(image)\n",
    "    crossing_counts_rows_train[i], crossing_counts_cols_train[i] = crossing_counts(image)\n",
    "    pixel_count_train[i] = pixel_count(image)\n",
    "\n",
    "for i in range(num_val):\n",
    "    image = validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    bounding_box_ratio_val[i] = np.mean(bounding_box_ratio(image))\n",
    "    distance_from_center_val[i] = distance_from_center(image)\n",
    "    radial_moments_val[i] = radial_moments(image)\n",
    "    convexity_val[i] = convexity(image)\n",
    "    skeleton_endpoints_val[i], skeleton_crossings_val[i] = skeleton_features(image)\n",
    "    crossing_counts_rows_val[i], crossing_counts_cols_val[i] = crossing_counts(image)\n",
    "    pixel_count_val[i] = pixel_count(image)\n",
    "\n",
    "for i in range(num_test):\n",
    "    image = test_images_tensor[i].reshape(28, 28).numpy()\n",
    "    bounding_box_ratio_test[i] = np.mean(bounding_box_ratio(image))\n",
    "    distance_from_center_test[i] = distance_from_center(image)\n",
    "    radial_moments_test[i] = radial_moments(image)\n",
    "    convexity_test[i] = convexity(image)\n",
    "    skeleton_endpoints_test[i], skeleton_crossings_test[i] = skeleton_features(image)\n",
    "    crossing_counts_rows_test[i], crossing_counts_cols_test[i] = crossing_counts(image)\n",
    "    pixel_count_test[i] = pixel_count(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([28, 28])\n",
      "Label tensor(0)\n",
      "Aspect ratio 0.7\n",
      "Number of black regions 2.0\n",
      "Eccentricity 0.759499675167631\n",
      "Extent 0.6035714285714285\n",
      "Edge density 31.224489795918366\n",
      "Circularity 16.908445114172334\n",
      "Bounding Box Ratio: 0.9642857142857143\n",
      "Distance from Center: 11.968069434419805\n",
      "Radial Moments (Mean, Variance, Skewness): [11.96806943 12.19620832 -0.75905868]\n",
      "Convexity: 0.9706666666666667\n",
      "Skeleton Endpoints: 40.0\n",
      "Skeleton Crossings: 53.0\n",
      "Pixel Count: 615.0\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "\n",
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image.shape)\n",
    "\n",
    "print(\"Label\", train_labels_tensor[image_id])\n",
    "print(\"Aspect ratio\", aspect_ratio_train[image_id])\n",
    "print(\"Number of black regions\", num_white_regions_train[image_id])\n",
    "print(\"Eccentricity\", eccentricity_train[image_id])\n",
    "print(\"Extent\", extent_train[image_id])\n",
    "print(\"Edge density\", edge_density_train[image_id])\n",
    "print(\"Circularity\", circularity_train[image_id])\n",
    "\n",
    "print(\"Bounding Box Ratio:\", bounding_box_ratio_train[image_id])\n",
    "print(\"Distance from Center:\", distance_from_center_train[image_id])\n",
    "print(\"Radial Moments (Mean, Variance, Skewness):\", radial_moments_train[image_id])\n",
    "print(\"Convexity:\", convexity_train[image_id])\n",
    "print(\"Skeleton Endpoints:\", skeleton_endpoints_train[image_id])\n",
    "print(\"Skeleton Crossings:\", skeleton_crossings_train[image_id])\n",
    "print(\"Pixel Count:\", pixel_count_train[image_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of new features\n",
    "num_new_features = 9\n",
    "\n",
    "features_train = np.zeros((num_train, 2 + num_new_features))\n",
    "features_val = np.zeros((num_val, 2 + num_new_features))\n",
    "features_test = np.zeros((num_test, 2 + num_new_features))\n",
    "\n",
    "# Processing the training dataset\n",
    "for i in range(num_train):\n",
    "    radial_features = radial_moments_train[i]  # this is assumed to be a tuple or list\n",
    "    new_features = [\n",
    "        bounding_box_ratio_train[i],\n",
    "        distance_from_center_train[i],\n",
    "        *radial_features,  # unpack the tuple or list\n",
    "        convexity_train[i],\n",
    "        skeleton_endpoints_train[i],\n",
    "        skeleton_crossings_train[i],\n",
    "        pixel_count_train[i]\n",
    "    ]\n",
    "    features_train[i, :] = np.hstack(\n",
    "        (aspect_ratio_train[i], num_white_regions_train[i], new_features)\n",
    "    )\n",
    "\n",
    "# Processing the validation dataset\n",
    "for i in range(num_val):\n",
    "    radial_features = radial_moments_val[i]  # assuming similar structure as train\n",
    "    new_features = [\n",
    "        bounding_box_ratio_val[i],\n",
    "        distance_from_center_val[i],\n",
    "        *radial_features,\n",
    "        convexity_val[i],\n",
    "        skeleton_endpoints_val[i],\n",
    "        skeleton_crossings_val[i],\n",
    "        pixel_count_val[i]\n",
    "    ]\n",
    "    features_val[i, :] = np.hstack(\n",
    "        (aspect_ratio_val[i], num_white_regions_val[i], new_features)\n",
    "    )\n",
    "\n",
    "# Processing the test dataset\n",
    "for i in range(num_test):\n",
    "    radial_features = radial_moments_test[i]  # assuming similar structure as train\n",
    "    new_features = [\n",
    "        bounding_box_ratio_test[i],\n",
    "        distance_from_center_test[i],\n",
    "        *radial_features,\n",
    "        convexity_test[i],\n",
    "        skeleton_endpoints_test[i],\n",
    "        skeleton_crossings_test[i],\n",
    "        pixel_count_test[i]\n",
    "    ]\n",
    "    features_test[i, :] = np.hstack(\n",
    "        (aspect_ratio_test[i], num_white_regions_test[i], new_features)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.7          2.           0.96428571  11.96806943  11.96806943\n",
      "  12.19620832  -0.75905868   0.97066667  40.          53.\n",
      " 615.        ]\n"
     ]
    }
   ],
   "source": [
    "print(features_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = torch.tensor(scaler.fit_transform(features_train))\n",
    "val_features_normalized = torch.tensor(scaler.transform(features_val))\n",
    "test_features_normalized = torch.tensor(scaler.transform(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = train_features_normalized.float()\n",
    "val_features_normalized = val_features_normalized.float()\n",
    "test_features_normalized = test_features_normalized.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a neural network, on the new 20x20 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "clf.fit(train_features_normalized, train_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python accuracy: 56.11000000000001 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_test_samples = len(test_features_normalized)\n",
    "python_predictions = clf.predict(test_features_normalized)\n",
    "python_accuracy = np.sum(python_predictions == test_labels) / num_test_samples\n",
    "print(f\"Python accuracy: {100*python_accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.08131283e-04 1.29506564e-01 0.00000000e+00 0.00000000e+00\n",
      " 4.09134592e-04 5.02246077e-02 2.92511197e-02 3.99538331e-01\n",
      " 3.74573908e-01 1.61882047e-02 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "importances = clf.feature_importances_\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=11, n_estimators=20, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=11, n_estimators=20, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=11, n_estimators=20, random_state=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=20, max_depth=11, random_state=0)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_clf.fit(train_features_normalized, train_labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python accuracy: 70.16 %\n"
     ]
    }
   ],
   "source": [
    "python_predictions = rf_clf.predict(test_features_normalized)\n",
    "python_accuracy = np.sum(python_predictions == test_labels) / num_test_samples\n",
    "print(f\"Python accuracy: {100*python_accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define code to measure accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, resized=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        if not resized:\n",
    "            test_outputs = model(test_features_normalized)\n",
    "        else:\n",
    "            test_outputs = model(test_features_resized_normalized)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 1.9887, validation loss: 1.9886\n",
      "Epoch [200], Loss: 1.6003, validation loss: 1.6058\n",
      "Epoch [300], Loss: 1.3646, validation loss: 1.3744\n",
      "Epoch [400], Loss: 1.2223, validation loss: 1.2316\n",
      "Epoch [500], Loss: 1.1380, validation loss: 1.1462\n",
      "Epoch [600], Loss: 1.0850, validation loss: 1.0911\n",
      "Epoch [700], Loss: 1.0449, validation loss: 1.0490\n",
      "Epoch [800], Loss: 1.0118, validation loss: 1.0140\n",
      "Epoch [900], Loss: 0.9833, validation loss: 0.9837\n",
      "Epoch [1000], Loss: 0.9590, validation loss: 0.9577\n",
      "Epoch [1100], Loss: 0.9382, validation loss: 0.9353\n",
      "Epoch [1200], Loss: 0.9209, validation loss: 0.9169\n",
      "Epoch [1300], Loss: 0.9071, validation loss: 0.9027\n",
      "Epoch [1400], Loss: 0.8957, validation loss: 0.8917\n",
      "Epoch [1500], Loss: 0.8864, validation loss: 0.8829\n",
      "Epoch [1600], Loss: 0.8784, validation loss: 0.8754\n",
      "Epoch [1700], Loss: 0.8715, validation loss: 0.8688\n",
      "Epoch [1800], Loss: 0.8654, validation loss: 0.8629\n",
      "Epoch [1900], Loss: 0.8599, validation loss: 0.8575\n",
      "Epoch [2000], Loss: 0.8547, validation loss: 0.8525\n",
      "Epoch [2100], Loss: 0.8498, validation loss: 0.8475\n",
      "Epoch [2200], Loss: 0.8450, validation loss: 0.8425\n",
      "Epoch [2300], Loss: 0.8404, validation loss: 0.8376\n",
      "Epoch [2400], Loss: 0.8358, validation loss: 0.8328\n",
      "Epoch [2500], Loss: 0.8313, validation loss: 0.8282\n",
      "Epoch [2600], Loss: 0.8268, validation loss: 0.8236\n",
      "Epoch [2700], Loss: 0.8224, validation loss: 0.8194\n",
      "Epoch [2800], Loss: 0.8183, validation loss: 0.8153\n",
      "Epoch [2900], Loss: 0.8143, validation loss: 0.8113\n",
      "Epoch [3000], Loss: 0.8104, validation loss: 0.8073\n",
      "Epoch [3100], Loss: 0.8067, validation loss: 0.8035\n",
      "Epoch [3200], Loss: 0.8031, validation loss: 0.7998\n",
      "Epoch [3300], Loss: 0.7998, validation loss: 0.7964\n",
      "Epoch [3400], Loss: 0.7968, validation loss: 0.7931\n",
      "Epoch [3500], Loss: 0.7941, validation loss: 0.7899\n",
      "Epoch [3600], Loss: 0.7914, validation loss: 0.7866\n",
      "Epoch [3700], Loss: 0.7889, validation loss: 0.7835\n",
      "Epoch [3800], Loss: 0.7866, validation loss: 0.7804\n",
      "Epoch [3900], Loss: 0.7844, validation loss: 0.7773\n",
      "Epoch [4000], Loss: 0.7823, validation loss: 0.7744\n",
      "Epoch [4100], Loss: 0.7803, validation loss: 0.7715\n",
      "Epoch [4200], Loss: 0.7783, validation loss: 0.7688\n",
      "Epoch [4300], Loss: 0.7764, validation loss: 0.7661\n",
      "Epoch [4400], Loss: 0.7746, validation loss: 0.7636\n",
      "Epoch [4500], Loss: 0.7728, validation loss: 0.7610\n",
      "Epoch [4600], Loss: 0.7710, validation loss: 0.7588\n",
      "Epoch [4700], Loss: 0.7694, validation loss: 0.7567\n",
      "Epoch [4800], Loss: 0.7678, validation loss: 0.7548\n",
      "Epoch [4900], Loss: 0.7663, validation loss: 0.7529\n",
      "Epoch [5000], Loss: 0.7647, validation loss: 0.7512\n",
      "Epoch [5100], Loss: 0.7630, validation loss: 0.7496\n",
      "Epoch [5200], Loss: 0.7615, validation loss: 0.7482\n",
      "Epoch [5300], Loss: 0.7598, validation loss: 0.7469\n",
      "Epoch [5400], Loss: 0.7583, validation loss: 0.7454\n",
      "Epoch [5500], Loss: 0.7568, validation loss: 0.7440\n",
      "Epoch [5600], Loss: 0.7553, validation loss: 0.7425\n",
      "Epoch [5700], Loss: 0.7539, validation loss: 0.7413\n",
      "Epoch [5800], Loss: 0.7526, validation loss: 0.7403\n",
      "Epoch [5900], Loss: 0.7514, validation loss: 0.7392\n",
      "Epoch [6000], Loss: 0.7504, validation loss: 0.7382\n",
      "Epoch [6100], Loss: 0.7493, validation loss: 0.7372\n",
      "Epoch [6200], Loss: 0.7483, validation loss: 0.7363\n",
      "Epoch [6300], Loss: 0.7473, validation loss: 0.7353\n",
      "Epoch [6400], Loss: 0.7465, validation loss: 0.7345\n",
      "Epoch [6500], Loss: 0.7456, validation loss: 0.7336\n",
      "Epoch [6600], Loss: 0.7449, validation loss: 0.7328\n",
      "Epoch [6700], Loss: 0.7442, validation loss: 0.7321\n",
      "Epoch [6800], Loss: 0.7435, validation loss: 0.7313\n",
      "Epoch [6900], Loss: 0.7428, validation loss: 0.7307\n",
      "Epoch [7000], Loss: 0.7422, validation loss: 0.7302\n",
      "Epoch [7100], Loss: 0.7417, validation loss: 0.7297\n",
      "Epoch [7200], Loss: 0.7412, validation loss: 0.7292\n",
      "Epoch [7300], Loss: 0.7408, validation loss: 0.7288\n",
      "Early stopping\n",
      "Accuracy: 0.74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the PyTorch neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "hidden_dim = int((input_dim + output_dim) / 2)\n",
    "\n",
    "# Instantiate the model\n",
    "model_medium2 = SimpleNN(\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_medium2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "validation_losses = []\n",
    "epoch = 0\n",
    "\n",
    "model_states = []\n",
    "\n",
    "while True:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_medium2(train_features_normalized)\n",
    "\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Add L1 regularization\n",
    "    l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "    for param in model_medium2.parameters():\n",
    "        l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    loss += lambda_l1 * l1_reg\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # store model state\n",
    "    model_states.append(model_medium2.state_dict())\n",
    "\n",
    "    # Compute validation loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model_medium2(val_features_normalized)\n",
    "        loss = criterion(outputs, validation_labels_tensor)\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "    if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "best_model_state = model_states[np.argmin(validation_losses)]\n",
    "model_medium2.load_state_dict(best_model_state)\n",
    "\n",
    "evaluate_model(model_medium2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
