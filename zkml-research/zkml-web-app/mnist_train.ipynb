{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of an MLP neural network Leo transpilation - MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP neural networks are expressive ML models. Here, we use them for the MNIST classification task, which contains images of handwritten digits. We show the inference of MLP neural networks for the MNIST dataset is possible in a zero knowledge environment.\n",
    "\n",
    "For this, we first download the dataset, and then compute feature representations of the dataset. We then train and test an MLP neural network on the feature dataset using PyTorch (because we can train with L1 regularization which is helpful for pruning later on). Afterward, we iteratively prune the network (meaning we set weights and biases close to 0 to actually 0, this will save circuitconstraints later on) and fine-tune it. Then, we convert the final PyTorch MLP model to a scikit-learn MLP model (since the transpiler supports scikit-learn models) and transpile the final MLP neural network to Leo, evaluate the Leo network and create a zero knowledge proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(url, save_path, folder_path):\n",
    "    \"\"\"Download and extract dataset if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        decompressed_file_name = os.path.splitext(os.path.basename(save_path))[0]\n",
    "        decompressed_file_path = os.path.join(folder_path, decompressed_file_name)\n",
    "\n",
    "        with gzip.open(save_path, \"rb\") as f_in:\n",
    "            with open(decompressed_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        print(f\"{decompressed_file_name} downloaded and extracted.\")\n",
    "    else:\n",
    "        print(f\"{os.path.basename(save_path)} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz already exists.\n",
      "train-labels-idx1-ubyte.gz already exists.\n",
      "t10k-images-idx3-ubyte.gz already exists.\n",
      "t10k-labels-idx1-ubyte.gz already exists.\n"
     ]
    }
   ],
   "source": [
    "# URLs and filenames\n",
    "file_info = [\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "        \"train-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "        \"train-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "        \"t10k-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
    "        \"t10k-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "folder_name = \"tmp/mnist\"\n",
    "folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Download and extract each file\n",
    "for url, file_name in file_info:\n",
    "    path_to_save = os.path.join(folder_path, file_name)\n",
    "    download_and_extract_dataset(url, path_to_save, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_idx3_ubyte_image_file(filename):\n",
    "    \"\"\"Read IDX3-ubyte formatted image data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_images = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2051:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        images = np.zeros((num_images, num_rows, num_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for r in range(num_rows):\n",
    "                for c in range(num_cols):\n",
    "                    pixel = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "                    images[i, r, c] = pixel\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_idx1_ubyte_label_file(filename):\n",
    "    \"\"\"Read IDX1-ubyte formatted label data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_labels = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2049:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        labels = np.zeros(num_labels, dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            labels[i] = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_images: (60000, 28, 28)\n",
      "Shape of train_labels: (60000,)\n",
      "Shape of test_images: (10000, 28, 28)\n",
      "Shape of test_labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(\n",
    "    os.getcwd(), folder_name\n",
    ")  # Adjust this path to where you stored the files\n",
    "\n",
    "train_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"train-images-idx3-ubyte\")\n",
    ")\n",
    "train_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"train-labels-idx1-ubyte\")\n",
    ")\n",
    "test_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"t10k-images-idx3-ubyte\")\n",
    ")\n",
    "test_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"t10k-labels-idx1-ubyte\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shape of train_images: {train_images.shape}\"\n",
    ")  # Should output \"Shape of train_images: (60000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of train_labels: {train_labels.shape}\"\n",
    ")  # Should output \"Shape of train_labels: (60000,)\"\n",
    "print(\n",
    "    f\"Shape of test_images: {test_images.shape}\"\n",
    ")  # Should output \"Shape of test_images: (10000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of test_labels: {test_labels.shape}\"\n",
    ")  # Should output \"Shape of test_labels: (10000,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the datasets to PyTorch tensors, and get a validation set\n",
    "(We use PyTorch instead of sci-kit learn to train sparse neural networks with L1 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertto pytorch tensors\n",
    "import torch\n",
    "\n",
    "train_images_tensor_initial = torch.from_numpy(train_images).float()\n",
    "train_labels_tensor_initial = torch.from_numpy(train_labels).long()\n",
    "test_images_tensor = torch.from_numpy(test_images).float()\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "\n",
    "# seed the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# shuffle the training dataset\n",
    "indices = torch.randperm(train_images_tensor_initial.shape[0])\n",
    "train_images_tensor_shuffled = train_images_tensor_initial[indices]\n",
    "train_labels_tensor_shuffled = train_labels_tensor_initial[indices]\n",
    "\n",
    "# get a 10% validation set\n",
    "validation_size = int(train_images_tensor_shuffled.shape[0] * 0.1)\n",
    "validation_images_tensor = train_images_tensor_shuffled[:validation_size]\n",
    "validation_labels_tensor = train_labels_tensor_shuffled[:validation_size]\n",
    "train_images_tensor = train_images_tensor_shuffled[validation_size:]\n",
    "train_labels_tensor = train_labels_tensor_shuffled[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels tensor shape: torch.Size([54000])\n",
      "Validation labels tensor shape: torch.Size([6000])\n",
      "Test labels tensor shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels tensor shape:\", train_labels_tensor.shape)\n",
    "print(\"Validation labels tensor shape:\", validation_labels_tensor.shape)\n",
    "print(\"Test labels tensor shape:\", test_labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0., 113., 255., 248.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0., 117., 248., 253., 246.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0., 117., 246., 253., 253., 248.,  39.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          24., 233., 253., 253., 253., 253., 223.,  40.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18.,\n",
      "         129., 253., 253., 228., 147., 253., 253., 164.,   5.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1., 137.,\n",
      "         253., 253., 212.,  28.,  12., 188., 253., 253., 116.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  14., 253.,\n",
      "         253., 253., 184.,   0.,   0.,  74., 253., 253., 170.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  15., 192., 253.,\n",
      "         253., 214.,  30.,   0.,   0.,   7., 253., 253., 251.,  53.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  35., 253., 253.,\n",
      "         253.,  41.,   0.,   0.,   0.,   7., 253., 253., 253.,  54.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 146., 253., 253.,\n",
      "         127.,   5.,   0.,   0.,   0.,   7., 253., 253., 253.,  54.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  41., 231., 253., 157.,\n",
      "           2.,   0.,   0.,   0.,   0., 109., 253., 253., 253.,  54.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 145., 253., 252., 110.,\n",
      "           0.,   0.,   0.,   0.,   3., 156., 253., 253., 199.,  19.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 192., 253., 239.,   0.,\n",
      "           0.,   0.,   0.,   0., 107., 253., 253., 253.,  93.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 192., 253., 239.,   0.,\n",
      "           0.,   0.,   0.,   0., 165., 253., 253., 203.,  17.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 192., 253., 239.,   0.,\n",
      "           0.,   0.,   0., 103., 249., 253., 253., 150.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 192., 253., 120.,   0.,\n",
      "           0.,   0.,  68., 208., 253., 253., 213.,  27.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 192., 253., 210.,   0.,\n",
      "           0., 105., 243., 253., 253., 248.,  28.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 192., 253., 249., 179.,\n",
      "         179., 245., 253., 253., 211.,  73.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 103., 246., 253., 253.,\n",
      "         253., 253., 199.,  96.,  34.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  79., 116., 186.,\n",
      "         253., 130.,  22.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "print(train_images_tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature representations of the dataset\n",
    "\n",
    "(We transform the bounding box images to 12x12 images, defined by the new_size variable. There is a trade-off in circuit constraints and ML model accuracy. You can increase the image size which will lead to a higher accuracy at the cost of more circuit constraints and thus longer proving times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_bounding_box(img):\n",
    "    \"\"\"\n",
    "    Extract the bounding box from an MNIST image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): 2D numpy array representing the MNIST image.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): Cropped image with the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert torch image to numpy array\n",
    "    img = img.numpy()\n",
    "\n",
    "    # Find the rows and columns where the image has non-zero pixels\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "\n",
    "    # Find the first and last row and column indices where the image has non-zero pixels\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return the cropped image\n",
    "    return img[rmin : rmax + 1, cmin : cmax + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, new_size):\n",
    "    height, width = image.shape\n",
    "    resized_image = np.zeros((new_size, new_size), dtype=np.uint8)\n",
    "\n",
    "    x_scale = width / new_size\n",
    "    y_scale = height / new_size\n",
    "\n",
    "    for i in range(new_size):\n",
    "        for j in range(new_size):\n",
    "            x_start = j * x_scale\n",
    "            y_start = i * y_scale\n",
    "            x_end = (j + 1) * x_scale\n",
    "            y_end = (i + 1) * y_scale\n",
    "\n",
    "            sum_pixels = 0\n",
    "            count = 0\n",
    "\n",
    "            for y in range(int(y_start), int(y_end)):\n",
    "                for x in range(int(x_start), int(x_end)):\n",
    "                    if 0 <= y < height and 0 <= x < width:\n",
    "                        sum_pixels += image[y, x]\n",
    "                        count += 1\n",
    "\n",
    "            resized_image[i, j] = round(sum_pixels / count) if count > 0 else 0\n",
    "\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_test_image_0 = None\n",
    "\n",
    "def get_resized_datasets(\n",
    "    train_images_tensor, validation_images_tensor, test_images_tensor, new_size\n",
    "):\n",
    "    num_train = len(train_images_tensor)\n",
    "    num_test = len(test_images_tensor)\n",
    "    num_val = len(validation_images_tensor)\n",
    "\n",
    "    train_images_tensor_resized = np.zeros((num_train, new_size**2))\n",
    "    validation_images_tensor_resized = np.zeros((num_val, new_size**2))\n",
    "    test_images_tensor_resized = np.zeros((num_test, new_size**2))\n",
    "\n",
    "    for i in range(num_train):\n",
    "        cropped_image = get_bounding_box(train_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = resize_image(\n",
    "            cropped_image_uint8, new_size\n",
    "        )\n",
    "        train_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    for i in range(num_val):\n",
    "        cropped_image = get_bounding_box(validation_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = resize_image(\n",
    "            cropped_image_uint8, new_size\n",
    "        )\n",
    "        validation_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    for i in range(num_test):\n",
    "        cropped_image = get_bounding_box(test_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        if(i == 0):\n",
    "            global cropped_test_image_0\n",
    "            cropped_test_image_0 = cropped_image_uint8\n",
    "        resized_image = resize_image(\n",
    "            cropped_image_uint8, new_size\n",
    "        )\n",
    "        test_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    return (\n",
    "        train_images_tensor_resized,\n",
    "        validation_images_tensor_resized,\n",
    "        test_images_tensor_resized,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = 12\n",
    "train_images_resized, val_images_resized, test_images_resized = get_resized_datasets(\n",
    "    train_images_tensor, validation_images_tensor, test_images_tensor, new_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_integral_image(image):\n",
    "    integral_image = np.cumsum(np.cumsum(image, axis=0), axis=1)\n",
    "    return integral_image\n",
    "\n",
    "def sum_region(integral_image, top_left, bottom_right):\n",
    "    # Sum of region using the integral image\n",
    "    tl_y, tl_x = top_left\n",
    "    br_y, br_x = bottom_right\n",
    "    \n",
    "    total = integral_image[br_y, br_x]\n",
    "    if tl_y > 0:\n",
    "        total -= integral_image[tl_y - 1, br_x]\n",
    "    if tl_x > 0:\n",
    "        total -= integral_image[br_y, tl_x - 1]\n",
    "    if tl_y > 0 and tl_x > 0:\n",
    "        total += integral_image[tl_y - 1, tl_x - 1]\n",
    "    \n",
    "    return total\n",
    "\n",
    "def compute_haar_features(image):\n",
    "    if image.shape[0] != image.shape[1]:\n",
    "        raise ValueError(\"The input image must be square.\")\n",
    "\n",
    "    integral_image = compute_integral_image(image)\n",
    "    features = []\n",
    "\n",
    "    for i in range(0, image.shape[0], 3):  # Slide vertically with a step of 3\n",
    "        for j in range(0, image.shape[0], 3):  # Slide horizontally with a step of 3\n",
    "\n",
    "            if i + 6 > image.shape[0] or j + 6 > image.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Horizontal feature\n",
    "            horizontal_feature_value = sum_region(integral_image, (i, j), (i+2, j+5)) - sum_region(integral_image, (i+3, j), (i+5, j+5))\n",
    "\n",
    "            # Vertical feature\n",
    "            vertical_feature_value = sum_region(integral_image, (i, j), (i+5, j+2)) - sum_region(integral_image, (i, j+3), (i+5, j+5))\n",
    "\n",
    "            features.append(horizontal_feature_value)\n",
    "            features.append(vertical_feature_value)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def aspect_ratio(image, threshold=0.5):\n",
    "    # Threshold the image to create a binary representation\n",
    "    bin_image = image > threshold\n",
    "    # Find the bounding box\n",
    "    row_indices, col_indices = np.nonzero(bin_image)\n",
    "    max_row, min_row = np.max(row_indices), np.min(row_indices)\n",
    "    max_col, min_col = np.max(col_indices), np.min(col_indices)\n",
    "\n",
    "    # Calculate the aspect ratio of the bounding box\n",
    "    width = max_col - min_col + 1\n",
    "    height = max_row - min_row + 1\n",
    "\n",
    "    if height == 0:  # To avoid division by zero\n",
    "        return 1.0\n",
    "\n",
    "    return width / height\n",
    "\n",
    "\n",
    "from scipy.ndimage import label\n",
    "\n",
    "\n",
    "def num_regions_below_threshold(image, threshold=0.5):\n",
    "    # Threshold the image so that pixels below the threshold are set to 1\n",
    "    # and those above the threshold are set to 0.\n",
    "    bin_image = image < threshold\n",
    "\n",
    "    # Use connected components labeling\n",
    "    labeled_array, num_features = label(bin_image)\n",
    "\n",
    "    # Return the number of unique regions\n",
    "    # (subtracting 1 as one of the labels will be the background)\n",
    "    return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "num_train = len(train_images_tensor)\n",
    "num_val = len(validation_images_tensor)\n",
    "num_test = len(test_images_tensor)\n",
    "\n",
    "aspect_ratio_train = np.zeros(num_train)\n",
    "aspect_ratio_val = np.zeros(num_val)\n",
    "aspect_ratio_test = np.zeros(num_test)\n",
    "\n",
    "num_white_regions_train = np.zeros(num_train)\n",
    "num_white_regions_val = np.zeros(num_val)\n",
    "num_white_regions_test = np.zeros(num_test)\n",
    "\n",
    "haar_1 = compute_haar_features(train_images_resized[0].reshape(new_size, new_size))\n",
    "len_haar_features = len(haar_1)\n",
    "\n",
    "haar_train = np.zeros((num_train, len_haar_features))\n",
    "haar_val = np.zeros((num_val, len_haar_features))\n",
    "haar_test = np.zeros((num_test, len_haar_features))\n",
    "\n",
    "for i in range(num_train):\n",
    "    aspect_ratio_train[i] = aspect_ratio(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_train[i] = num_regions_below_threshold(\n",
    "        train_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_train[i, :] = compute_haar_features(\n",
    "        train_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    aspect_ratio_val[i] = aspect_ratio(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    num_white_regions_val[i] = num_regions_below_threshold(\n",
    "        validation_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_val[i, :] = compute_haar_features(\n",
    "        val_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    aspect_ratio_test[i] = aspect_ratio(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_test[i] = num_regions_below_threshold(\n",
    "        test_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_test[i, :] = compute_haar_features(\n",
    "        test_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the images, and the computed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([28, 28])\n",
      "Label tensor(0)\n",
      "Resized image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (12, 12)\n",
      "Haar features: [-1755. -1751.   484.  -128.   297.   125.   118.   364.   934.   812.\n",
      "  -434. -2370.  -598.  2250. -1380. -1040.   879.   373.]\n",
      "Shape of Haar features: (18,)\n",
      "Aspect ratio: 0.7\n",
      "Number of white regions: 2.0\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "\n",
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "\n",
    "print(\"Original image\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image.shape)\n",
    "print(\"Label\", train_labels_tensor[image_id])\n",
    "\n",
    "print(\"Resized image\")\n",
    "\n",
    "image_resized = train_images_resized[image_id].reshape(new_size, new_size)\n",
    "\n",
    "plt.imshow(image_resized, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image_resized.shape)\n",
    "\n",
    "print(\"Haar features:\", haar_train[image_id, :])\n",
    "print(\"Shape of Haar features:\", haar_train[image_id, :].shape)\n",
    "\n",
    "print(\"Aspect ratio:\", aspect_ratio_train[image_id])\n",
    "print(\"Number of white regions:\", num_white_regions_train[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's merge all features into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "train_features = np.zeros((num_train, len_haar_features + 2))\n",
    "val_features = np.zeros((num_val, len_haar_features + 2))\n",
    "test_features = np.zeros((num_test, len_haar_features + 2))\n",
    "\n",
    "for i in range(num_train):\n",
    "    train_features[i, :] = np.hstack(\n",
    "        (haar_train[i, :], aspect_ratio_train[i], num_white_regions_train[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    val_features[i, :] = np.hstack(\n",
    "        (haar_val[i, :], aspect_ratio_val[i], num_white_regions_val[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_features[i, :] = np.hstack(\n",
    "        (haar_test[i, :], aspect_ratio_test[i], num_white_regions_test[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features: (54000, 20)\n",
      "First training feature vector: [-1.755e+03 -1.751e+03  4.840e+02 -1.280e+02  2.970e+02  1.250e+02\n",
      "  1.180e+02  3.640e+02  9.340e+02  8.120e+02 -4.340e+02 -2.370e+03\n",
      " -5.980e+02  2.250e+03 -1.380e+03 -1.040e+03  8.790e+02  3.730e+02\n",
      "  7.000e-01  2.000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training features:\", train_features.shape)\n",
    "print(\"First training feature vector:\", train_features[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [-156.89455556 -746.57240741  181.82409259 -233.12468519  215.89527778\n",
      "  263.25757407  -26.51775926 -543.36164815  -95.56633333 -166.66537037\n",
      "   12.45133333  338.88711111 -328.52153704 -618.70875926 -104.3222963\n",
      "   63.83240741  266.76446296  587.49375926    0.80027855    1.64944444]\n",
      "Standard deviation: [1.04394811e+03 9.22641317e+02 1.08172584e+03 9.85078247e+02\n",
      " 8.74984717e+02 1.12660933e+03 1.00850222e+03 1.12867243e+03\n",
      " 1.08860804e+03 1.05573917e+03 9.81360965e+02 1.34469105e+03\n",
      " 1.02263037e+03 9.90665097e+02 1.18404377e+03 1.17523292e+03\n",
      " 8.17505785e+02 1.20897397e+03 2.21488782e-01 8.68779298e-01]\n",
      "Mean: [-156.89455555555554, -746.5724074074074, 181.8240925925926, -233.12468518518517, 215.89527777777778, 263.25757407407406, -26.51775925925926, -543.3616481481481, -95.56633333333333, -166.66537037037037, 12.451333333333332, 338.8871111111111, -328.52153703703704, -618.7087592592593, -104.3222962962963, 63.83240740740741, 266.76446296296297, 587.4937592592593, 0.800278554246832, 1.6494444444444445]\n",
      "Standard deviation: [1043.9481099421746, 922.6413166564906, 1081.7258364235456, 985.0782471787254, 874.9847165310242, 1126.609325302833, 1008.5022217328883, 1128.672429354368, 1088.6080378204506, 1055.7391728007258, 981.360965040711, 1344.6910499347453, 1022.6303743646741, 990.6650968719681, 1184.0437747222106, 1175.2329215102861, 817.5057847253131, 1208.9739683033454, 0.22148878238313918, 0.8687792982892613]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = torch.tensor(scaler.fit_transform(train_features)).float()\n",
    "val_features_normalized = torch.tensor(scaler.transform(val_features)).float()\n",
    "test_features_normalized = torch.tensor(scaler.transform(test_features)).float()\n",
    "\n",
    "# print normalizing parameters\n",
    "print(\"Mean:\", scaler.mean_)\n",
    "print(\"Standard deviation:\", scaler.scale_)\n",
    "\n",
    "# print mean as javascript array\n",
    "print(\"Mean:\", scaler.mean_.tolist())\n",
    "\n",
    "# print standard deviation as javascript array\n",
    "print(\"Standard deviation:\", scaler.scale_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training feature vector (normalized): tensor([-1.5308, -1.0886,  0.2793,  0.1067,  0.0927, -0.1227,  0.1433,  0.8039,\n",
      "         0.9458,  0.9270, -0.4549, -2.0145, -0.2635,  2.8957, -1.0774, -0.9392,\n",
      "         0.7489, -0.1774, -0.4527,  0.4035])\n"
     ]
    }
   ],
   "source": [
    "print(\"First training feature vector (normalized):\", train_features_normalized[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network and the training and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_labels):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_features_normalized)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Define the PyTorch neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_labels, val_labels):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop with L1 regularization\n",
    "    lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "    validation_losses = []\n",
    "    epoch = 0\n",
    "\n",
    "    model_states = []\n",
    "\n",
    "    while True:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_features_normalized)\n",
    "\n",
    "        loss = criterion(outputs, train_labels)\n",
    "\n",
    "        # Add L1 regularization\n",
    "        l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "        for param in model.parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        loss += lambda_l1 * l1_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "        # store model state\n",
    "        model_states.append(model.state_dict())\n",
    "\n",
    "        # Compute validation loss\n",
    "        with torch.no_grad():\n",
    "            outputs = model(val_features_normalized)\n",
    "            loss = criterion(outputs, val_labels)\n",
    "            validation_losses.append(loss.item())\n",
    "\n",
    "        # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "        if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    best_model_state = model_states[np.argmin(validation_losses)]\n",
    "    model.load_state_dict(best_model_state)\n",
    "    validation_loss_of_best_model = validation_losses[np.argmin(validation_losses)]\n",
    "\n",
    "    return model, epoch, validation_loss_of_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "hidden_dim = (input_dim + output_dim) // 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 1.6662, validation loss: 1.6651\n",
      "Epoch [200], Loss: 0.9860, validation loss: 0.9828\n",
      "Epoch [300], Loss: 0.6444, validation loss: 0.6410\n",
      "Epoch [400], Loss: 0.4980, validation loss: 0.4945\n",
      "Epoch [500], Loss: 0.4302, validation loss: 0.4264\n",
      "Epoch [600], Loss: 0.3938, validation loss: 0.3899\n",
      "Epoch [700], Loss: 0.3716, validation loss: 0.3674\n",
      "Epoch [800], Loss: 0.3566, validation loss: 0.3523\n",
      "Epoch [900], Loss: 0.3460, validation loss: 0.3416\n",
      "Epoch [1000], Loss: 0.3381, validation loss: 0.3337\n",
      "Epoch [1100], Loss: 0.3319, validation loss: 0.3276\n",
      "Epoch [1200], Loss: 0.3269, validation loss: 0.3226\n",
      "Epoch [1300], Loss: 0.3226, validation loss: 0.3181\n",
      "Epoch [1400], Loss: 0.3190, validation loss: 0.3141\n",
      "Epoch [1500], Loss: 0.3158, validation loss: 0.3107\n",
      "Epoch [1600], Loss: 0.3128, validation loss: 0.3075\n",
      "Epoch [1700], Loss: 0.3100, validation loss: 0.3045\n",
      "Epoch [1800], Loss: 0.3073, validation loss: 0.3017\n",
      "Epoch [1900], Loss: 0.3046, validation loss: 0.2989\n",
      "Epoch [2000], Loss: 0.3018, validation loss: 0.2962\n",
      "Epoch [2100], Loss: 0.2992, validation loss: 0.2937\n",
      "Epoch [2200], Loss: 0.2967, validation loss: 0.2911\n",
      "Epoch [2300], Loss: 0.2942, validation loss: 0.2886\n",
      "Epoch [2400], Loss: 0.2918, validation loss: 0.2861\n",
      "Epoch [2500], Loss: 0.2895, validation loss: 0.2836\n",
      "Epoch [2600], Loss: 0.2873, validation loss: 0.2811\n",
      "Epoch [2700], Loss: 0.2850, validation loss: 0.2784\n",
      "Epoch [2800], Loss: 0.2828, validation loss: 0.2759\n",
      "Epoch [2900], Loss: 0.2807, validation loss: 0.2734\n",
      "Epoch [3000], Loss: 0.2785, validation loss: 0.2710\n",
      "Epoch [3100], Loss: 0.2764, validation loss: 0.2685\n",
      "Epoch [3200], Loss: 0.2742, validation loss: 0.2660\n",
      "Epoch [3300], Loss: 0.2720, validation loss: 0.2638\n",
      "Epoch [3400], Loss: 0.2699, validation loss: 0.2616\n",
      "Epoch [3500], Loss: 0.2679, validation loss: 0.2593\n",
      "Epoch [3600], Loss: 0.2659, validation loss: 0.2571\n",
      "Epoch [3700], Loss: 0.2638, validation loss: 0.2547\n",
      "Epoch [3800], Loss: 0.2619, validation loss: 0.2522\n",
      "Epoch [3900], Loss: 0.2600, validation loss: 0.2500\n",
      "Epoch [4000], Loss: 0.2583, validation loss: 0.2479\n",
      "Epoch [4100], Loss: 0.2565, validation loss: 0.2460\n",
      "Epoch [4200], Loss: 0.2547, validation loss: 0.2441\n",
      "Epoch [4300], Loss: 0.2529, validation loss: 0.2421\n",
      "Epoch [4400], Loss: 0.2511, validation loss: 0.2403\n",
      "Epoch [4500], Loss: 0.2494, validation loss: 0.2384\n",
      "Epoch [4600], Loss: 0.2477, validation loss: 0.2366\n",
      "Epoch [4700], Loss: 0.2461, validation loss: 0.2347\n",
      "Epoch [4800], Loss: 0.2446, validation loss: 0.2334\n",
      "Epoch [4900], Loss: 0.2430, validation loss: 0.2317\n",
      "Epoch [5000], Loss: 0.2416, validation loss: 0.2300\n",
      "Epoch [5100], Loss: 0.2404, validation loss: 0.2282\n",
      "Epoch [5200], Loss: 0.2393, validation loss: 0.2266\n",
      "Epoch [5300], Loss: 0.2381, validation loss: 0.2250\n",
      "Epoch [5400], Loss: 0.2370, validation loss: 0.2238\n",
      "Epoch [5500], Loss: 0.2359, validation loss: 0.2227\n",
      "Epoch [5600], Loss: 0.2348, validation loss: 0.2219\n",
      "Epoch [5700], Loss: 0.2339, validation loss: 0.2212\n",
      "Epoch [5800], Loss: 0.2332, validation loss: 0.2208\n",
      "Epoch [5900], Loss: 0.2325, validation loss: 0.2204\n",
      "Epoch [6000], Loss: 0.2319, validation loss: 0.2200\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model, epochs, val_loss = train(model, train_labels_tensor, validation_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9363"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_pytorch_network(\n",
    "    model, weight_threshold=1e-1, bias_threshold=1e-1\n",
    "):  # noqa: D103\n",
    "    num_weights = 0\n",
    "    num_weights_already_zero = 0\n",
    "    num_changed_weights = 0\n",
    "    num_biases = 0\n",
    "    num_biases_already_zero = 0\n",
    "    num_changed_biases = 0\n",
    "\n",
    "    # Pruning the weights\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            flattened_weights = param.data.view(-1)\n",
    "            for j, weight in enumerate(flattened_weights):\n",
    "                if weight == 0:\n",
    "                    num_weights_already_zero += 1\n",
    "                elif abs(weight) < weight_threshold:\n",
    "                    flattened_weights[j] = 0\n",
    "                    num_changed_weights += 1\n",
    "                num_weights += 1\n",
    "            param.data = flattened_weights.view(param.data.shape)\n",
    "        elif \"bias\" in name:\n",
    "            flattened_biases = param.data.view(-1)\n",
    "            for j, bias in enumerate(flattened_biases):\n",
    "                if bias == 0:\n",
    "                    num_biases_already_zero += 1\n",
    "                elif abs(bias) < bias_threshold:\n",
    "                    flattened_biases[j] = 0\n",
    "                    num_changed_biases += 1\n",
    "                num_biases += 1\n",
    "            param.data = flattened_biases.view(param.data.shape)\n",
    "\n",
    "    # Set gradients of pruned weights to zero\n",
    "    def zero_gradients_hook(grad):\n",
    "        return grad * (grad != 0)\n",
    "\n",
    "    hooks = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            hooks.append(param.register_hook(zero_gradients_hook))\n",
    "\n",
    "    print(f\"Number of weight parameters: {num_weights}\")  # noqa: T201\n",
    "    print(f\"Number of weights already zero: {num_weights_already_zero}\")  # noqa: T201\n",
    "    print(f\"Number of changed weight parameters: {num_changed_weights}\")  # noqa: T201\n",
    "    print(f\"Number of bias parameters: {num_biases}\")  # noqa: T201\n",
    "    print(f\"Number of biases already zero: {num_biases_already_zero}\")  # noqa: T201\n",
    "    print(f\"Number of changed bias parameters: {num_changed_biases}\")  # noqa: T201\n",
    "    print(  # noqa: T201\n",
    "        f\"Percentage of weights pruned: {num_changed_weights / num_weights * 100:.2f}%\"\n",
    "    )\n",
    "    print(  # noqa: T201\n",
    "        f\"Percentage of biases pruned: {num_changed_biases / num_biases * 100:.2f}%\"\n",
    "    )\n",
    "    print(  # noqa: T201\n",
    "        f\"Remaining number of non-zero weights: {num_weights - num_changed_weights}\"\n",
    "    )\n",
    "    print(  # noqa: T201\n",
    "        f\"Remaining number of non-zero biases: {num_biases - num_changed_biases}\"\n",
    "    )\n",
    "\n",
    "    return model, hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 123\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 4\n",
      "Percentage of weights pruned: 27.33%\n",
      "Percentage of biases pruned: 16.00%\n",
      "Remaining number of non-zero weights: 327\n",
      "Remaining number of non-zero biases: 21\n",
      "Accuracy: 0.9341\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model_pruned = copy.deepcopy(model)\n",
    "model_pruned, hooks = prune_pytorch_network(model_pruned, 1e-1, 1e-1)\n",
    "\n",
    "_ = evaluate_model(model_pruned, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a loop of fine-tuning, pruning, and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 123\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 4\n",
      "Percentage of weights pruned: 27.33%\n",
      "Percentage of biases pruned: 16.00%\n",
      "Remaining number of non-zero weights: 327\n",
      "Remaining number of non-zero biases: 21\n",
      "Now training\n",
      "Early stopping\n",
      "Iteration 1\n",
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 126\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 4\n",
      "Percentage of weights pruned: 28.00%\n",
      "Percentage of biases pruned: 16.00%\n",
      "Remaining number of non-zero weights: 324\n",
      "Remaining number of non-zero biases: 21\n",
      "Now training\n",
      "Epoch [100], Loss: 0.2307, validation loss: 0.2188\n",
      "Early stopping\n",
      "Iteration 2\n",
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 133\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 4\n",
      "Percentage of weights pruned: 29.56%\n",
      "Percentage of biases pruned: 16.00%\n",
      "Remaining number of non-zero weights: 317\n",
      "Remaining number of non-zero biases: 21\n",
      "Now training\n",
      "Early stopping\n",
      "Iteration 3\n",
      "Number of weight parameters: 450\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 134\n",
      "Number of bias parameters: 25\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 4\n",
      "Percentage of weights pruned: 29.78%\n",
      "Percentage of biases pruned: 16.00%\n",
      "Remaining number of non-zero weights: 316\n",
      "Remaining number of non-zero biases: 21\n",
      "Now training\n",
      "Early stopping\n",
      "Early stopping\n",
      "Accuracy: 0.9361\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "val_loss_new = val_loss\n",
    "model_to_prune = copy.deepcopy(model)\n",
    "\n",
    "while True:\n",
    "    print(f\"Iteration {i}\")\n",
    "    model_pruned = copy.deepcopy(model_to_prune)\n",
    "    model_pruned, _ = prune_pytorch_network(model_pruned, 1e-1, 1e-1)\n",
    "    val_loss_old = val_loss_new\n",
    "\n",
    "    model_to_prune = copy.deepcopy(model_pruned)\n",
    "    print(\"Now training\")\n",
    "    model_to_prune, _, val_loss_new = train(model_to_prune, train_labels_tensor, validation_labels_tensor)\n",
    "\n",
    "    if val_loss_new > val_loss_old:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "accuracy = evaluate_model(model_pruned, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert this model to a scikit-learn MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def pytorch_to_sklearn(pytorch_model):\n",
    "\n",
    "    # Extract weights and biases from PyTorch model\n",
    "    fc1_weight = pytorch_model.fc1.weight.data\n",
    "    fc1_bias = pytorch_model.fc1.bias.data\n",
    "    fc2_weight = pytorch_model.fc2.weight.data\n",
    "    fc2_bias = pytorch_model.fc2.bias.data\n",
    "\n",
    "    # Get the sizes for initialization\n",
    "    input_size = fc1_weight.shape[1]\n",
    "    hidden_size = fc1_weight.shape[0]\n",
    "    output_size = fc2_weight.shape[0]\n",
    "\n",
    "    # Initialize sklearn MLP\n",
    "    sklearn_mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(hidden_size,), activation=\"relu\", max_iter=1\n",
    "    )\n",
    "\n",
    "    # To ensure the model doesn't change the weights during the dummy fit, we set warm_start=True\n",
    "    sklearn_mlp.warm_start = True\n",
    "\n",
    "    # Dummy fit to initialize weights (necessary step before setting the weights)\n",
    "    sklearn_mlp.fit(np.zeros((output_size, input_size)), list(range(output_size)))\n",
    "\n",
    "    # Set the weights and biases\n",
    "    sklearn_mlp.coefs_[0] = fc1_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[0] = fc1_bias.numpy()\n",
    "    sklearn_mlp.coefs_[1] = fc2_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[1] = fc2_bias.numpy()\n",
    "\n",
    "    return sklearn_mlp\n",
    "\n",
    "\n",
    "# Convert the example PyTorch MLP to sklearn MLP\n",
    "converted_model = pytorch_to_sklearn(model_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9361\n",
      "Number of neurons per layer: [20, 15, 10]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the sklearn model\n",
    "accuracy = converted_model.score(test_features_normalized.numpy(), test_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "layers_sizes = [converted_model.coefs_[0].shape[0]] + [\n",
    "    coef.shape[1] for coef in converted_model.coefs_\n",
    "]\n",
    "\n",
    "print(\"Number of neurons per layer:\", layers_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's transpile this model to Leo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "Constraints: 147972\n",
      "Runtime for one instance: 3.6300060749053955 seconds\n",
      "Leo accuracy: 80.0 %\n",
      "Python accuracy: 80.0 %\n"
     ]
    }
   ],
   "source": [
    "from zkml import LeoTranspiler\n",
    "\n",
    "# Transpile the deceision tree into Leo code\n",
    "print(type(converted_model))\n",
    "lt = LeoTranspiler(\n",
    "    model=converted_model, validation_data=train_features_normalized[0:600].numpy()\n",
    ")\n",
    "leo_project_path = os.path.join(os.getcwd(), \"tmp\", \"mnist\")\n",
    "leo_project_name = \"sklearn_mlp_mnist_1\"\n",
    "lt.to_leo(\n",
    "    path=leo_project_path, project_name=leo_project_name, fixed_point_scaling_factor=16\n",
    ")\n",
    "\n",
    "# Compute the accuracy of the Leo program and the Python program on the test set\n",
    "num_test_samples = len(test_features)\n",
    "\n",
    "# let's limit the number of test stamples to 10 to make the computation faster\n",
    "num_test_samples = min(num_test_samples, 10)\n",
    "\n",
    "python_predictions = converted_model.predict(test_features)\n",
    "\n",
    "leo_predictions = np.zeros(num_test_samples)\n",
    "for i in range(num_test_samples):\n",
    "    lc = lt.run(input=test_features[i])\n",
    "    leo_predictions[i] = np.argmax(lc.output_decimal)\n",
    "\n",
    "print(f\"Constraints: {lc.circuit_constraints}\")\n",
    "print(f\"Runtime for one instance: {lc.runtime} seconds\")\n",
    "\n",
    "leo_accuracy = (\n",
    "    np.sum(leo_predictions[0:num_test_samples] == test_labels[0:num_test_samples])\n",
    "    / num_test_samples\n",
    ")\n",
    "python_accuracy = (\n",
    "    np.sum(python_predictions[0:num_test_samples] == test_labels[0:num_test_samples])\n",
    "    / num_test_samples\n",
    ")\n",
    "\n",
    "print(f\"Leo accuracy: {100*leo_accuracy} %\")\n",
    "print(f\"Python accuracy: {100*python_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's generate a proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints: 147972\n",
      "Runtime for one instance: 28.183155059814453 seconds\n",
      "\n",
      "Leo prediction: [-1457.217041015625, -5426.77490234375, 3321.30517578125, 2152.006103515625, -6994.791015625, -6128.408203125, -11780.537841796875, 5969.34619140625, 1985.5869140625, 3234.726806640625]\n",
      "Python prediction: 7\n",
      "True label: 7\n",
      "\n",
      "Proof: proof1qyqsqqqqqqqqqqqpqqqqqqqqqqqrwhjmms8lv52u43ff6a22mn3yfwwqv3jzg00fp0sy7tkv699f42gl50lwycl764ejy2p2vudj5tsqqy27szvhax3lwm6gs3t8lpf2dhxxx4jy2eywny9zafrxk46f2hjng7gmhy3pu07tewl6m49jjtkssqr6r27y52qxwf2wf9ft0r97c6e5xywggthwmla44e63fc8wldqd37aas4qsr50rxejrk0qwr2px2gqwhp76l8lrnsadsdca6a5jtxjgwsy0vf9tpdfmtj2s48rpdg4ax7l46wachcu236v22au0s8tp6rgqsnvzlgp9wuqulg2akcwendm4n5kq7has8pq5p9rcluqavzcvcxt4wytx5nxlfatl6j8vlxgneuccpeuhelgt78tlur6xgvh4fre7aecpadruycfw4se77hm5vttz3gpywwvm5f68p7wetvremk2a8048sxg6949t62ftwh5t34fk2qs56l0ecs2deevmlswk66xfpytlc52g4vlc7cgmc9mewt7jta4y5qax3qqyvpa3v3t7rm9wduqjkjdh0gsz4gvgxltugwwjvz6292xy8upxjfsddtfu2kzj3dxm8jk0u2qgy5q2jy7xx509r5xycups0czvk4gy8p0y3qjjzzc25rg8xptkl2nywem2dy5kesqc85cqxxr69pkh4h5q8a2vvedsfc4wwjydhe3g3h8repegn4548h4lwdkl0jvgdgm4gyrphmfc2tulz890w3yqzcvuql9cn942jln7qjyvqka7amv67xq6yqw6kgxgk9dg4q4gt0sc3aulh84kjcnaht6y89pywx5gs5t46yclpnecxjzlnz36f9ezz5pm8j8xcjt69gkyq9nemgnesh8pwrtrprspp09smvmq4py00znu3uy6wsw0xjdd7csk0uqam75uuqywmp74hhgr53tcas9nxn8ezuv9w5au80fjfat3rwkaa73r33da8t0cjdfwjyguqeezywcrf8nztqzhde020sq72mtntm8gnwvy6vk7j6lwyava2q9hp7upk0a8g9jv7c2k2ccyp5sv053a9frhp55l82dsea67f738pc42hgss8m8wt4wpz9kuzdmc7v8scg893xce0m7c7jlzkfh06thprdmggwtf5h598ug55x4t3y5epg5z0x4243cdyqpcvw7xs44wl7csqvqqqqqqqqqqp3lkjcljdsy7vqgw86u8kkwenxf9uytkpk2k5x3ntd62eulgqpq6ztpzg3dgwrm9x7xupmtvel0dsqq04h0n97qlxs6y760c94rd3h53gp0uuhzw689acy3fy89des3st8ahrsnphtuye3v3gua80h4ctruqqyc5gwkjru3sparw5x2zg3whtnnqekxgxu0fwp4m49r46lm9nmlsuawa3z7vdp0nk7p6dunh8mm07d5z24q20yf9cta3qppmeapskg8p56d4hgg7cpcy0gnsyey0gwk2sqqqqpmrqw\n"
     ]
    }
   ],
   "source": [
    "zkp = lt.execute(input=test_features[0])\n",
    "\n",
    "print(f\"Constraints: {zkp.circuit_constraints}\")\n",
    "print(f\"Runtime for one instance: {zkp.runtime} seconds\\n\")\n",
    "\n",
    "print(f\"Leo prediction: {zkp.output_decimal}\")\n",
    "print(f\"Python prediction: {python_predictions[0]}\")\n",
    "print(f\"True label: {test_labels[0]}\\n\")\n",
    "\n",
    "print(f\"Proof: {zkp.proof}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Plot the image\")\n",
    "plt.imshow(test_images_tensor[0].reshape(28, 28), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [ 1.711e+03  1.410e+02  1.228e+03 -3.260e+02  4.110e+02 -9.950e+02\n",
      " -1.290e+02 -1.290e+02 -1.066e+03 -1.476e+03  1.320e+02  5.360e+02\n",
      " -1.162e+03 -1.420e+03 -7.900e+01 -3.900e+01  1.083e+03  1.459e+03\n",
      "  8.000e-01  1.000e+00]\n"
     ]
    }
   ],
   "source": [
    "# print the featuers\n",
    "print(\"Features:\", test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Javascript array: [1711.0, 141.0, 1228.0, -326.0, 411.0, -995.0, -129.0, -129.0, -1066.0, -1476.0, 132.0, 536.0, -1162.0, -1420.0, -79.0, -39.0, 1083.0, 1459.0, 0.8, 1.0, ]\n"
     ]
    }
   ],
   "source": [
    "# convert test_features[0] into a javascript array\n",
    "js_array = \"[\"\n",
    "for i in range(len(test_features[0])):\n",
    "    js_array += str(test_features[0][i]) + \", \"\n",
    "js_array += \"]\"\n",
    "\n",
    "print(\"Javascript array:\", js_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 84.0, 185.0, 159.0, 151.0, 60.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 222.0, 254.0, 254.0, 254.0, 254.0, 241.0, 198.0, 198.0, 198.0, 198.0, 198.0, 198.0, 198.0, 198.0, 170.0, 52.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 67.0, 114.0, 72.0, 114.0, 163.0, 227.0, 254.0, 225.0, 254.0, 254.0, 254.0, 250.0, 229.0, 254.0, 254.0, 140.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.0, 66.0, 14.0, 67.0, 67.0, 67.0, 59.0, 21.0, 236.0, 254.0, 106.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 83.0, 253.0, 209.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 22.0, 233.0, 255.0, 83.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 129.0, 254.0, 238.0, 44.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.0, 249.0, 254.0, 62.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 133.0, 254.0, 187.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.0, 205.0, 248.0, 58.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 126.0, 254.0, 182.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 75.0, 251.0, 240.0, 57.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.0, 221.0, 254.0, 166.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 203.0, 254.0, 219.0, 35.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 38.0, 254.0, 254.0, 77.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 224.0, 254.0, 115.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 133.0, 254.0, 254.0, 52.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 61.0, 242.0, 254.0, 254.0, 52.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 121.0, 254.0, 254.0, 219.0, 40.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 121.0, 254.0, 207.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# convert test_images_tensor[0] into a javascript array\n",
    "js_array = \"[\"\n",
    "for item in test_images_tensor[0].tolist():\n",
    "    js_array += str(item) + \", \"\n",
    "js_array = js_array.rstrip(', ')  # Remove the trailing comma and space\n",
    "js_array += \"]\"\n",
    "\n",
    "print(js_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 84 185 159 151  60  36   0   0   0   0   0   0   0   0   0   0]\n",
      " [222 254 254 254 254 241 198 198 198 198 198 198 198 198 170  52]\n",
      " [ 67 114  72 114 163 227 254 225 254 254 254 250 229 254 254 140]\n",
      " [  0   0   0   0   0  17  66  14  67  67  67  59  21 236 254 106]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  83 253 209  18]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  22 233 255  83   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 129 254 238  44   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  59 249 254  62   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 133 254 187   5   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   9 205 248  58   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 126 254 182   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  75 251 240  57   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  19 221 254 166   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   3 203 254 219  35   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  38 254 254  77   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  31 224 254 115   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 133 254 254  52   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  61 242 254 254  52   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 121 254 254 219  40   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 121 254 207  18   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(cropped_test_image_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 84. 185. 155.  60.  36.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [144. 184. 174. 208. 234. 219. 226. 226. 225. 214. 226. 154.]\n",
      " [  0.   0.   0.   0.   8.  20.  34.  34.  32.  52. 244. 147.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  11. 233. 255.  42.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0. 109. 254. 150.  11.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   4. 210. 122.   2.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0. 126. 218.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   5. 148. 252. 116.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0. 124. 254. 148.   9.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  31. 239. 115.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.  30. 188. 254.  52.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0. 121. 254. 174.  20.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_images_resized[0].reshape(new_size, new_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=9, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=9, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=9, random_state=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=9, random_state=0)\n",
    "tree.fit(train_features_normalized, train_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for our baseline: 83.76%\n",
      "Validation accuracy for our baseline: 82.32%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred) * 100  # Multiply by 100 to get percentage\n",
    "\n",
    "# Evaluate the baseline model\n",
    "train_preds_baseline = tree.predict(train_features_normalized)\n",
    "val_preds_baseline = tree.predict(test_features_normalized)\n",
    "acc_baseline_train = acc(train_preds_baseline, train_labels_tensor)\n",
    "acc_baseline_val = acc(val_preds_baseline, test_labels_tensor)\n",
    "print(\n",
    "    \"Training accuracy for our baseline: \"\n",
    "    f\"{acc_baseline_train:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    \"Validation accuracy for our baseline: \"\n",
    "    f\"{acc_baseline_val:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing number ranges and fixed-point scaling factor...\n",
      "INFO:root:Minimum number: -3.603619337081909, maximum number: 5.416624069213867. Recommended fixed-point scaling factor: 16, required Leo type: i64\n",
      "INFO:root:Transpiling model...\n",
      "INFO:root:Leo program stored\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# Set the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "tree_transpiler = LeoTranspiler(model=tree, validation_data=train_features_normalized.numpy()[0:200])\n",
    "leo_project_path = os.path.join(os.getcwd(), \"tmp/mnist\")\n",
    "leo_project_name = \"tree_mnist_1\"\n",
    "tree_transpiler.to_leo(path=leo_project_path, project_name=leo_project_name, fixed_point_scaling_factor=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit constraints: 32305\n",
      "Runtime: 12.573861122131348 seconds\n",
      "Active input count: 20\n",
      "Leo prediction in fixed-point notation: 112\n",
      "Leo prediction in decimal notation: 7.0\n",
      "Python prediction: 7\n",
      "Label: 7\n"
     ]
    }
   ],
   "source": [
    "# prove and compare the Leo prediction with the Python prediction and the label\n",
    "zkp = tree_transpiler.execute(input=test_features_normalized.numpy()[0])\n",
    "python_prediction = tree.predict([test_features_normalized[0]])\n",
    "\n",
    "print(f\"Circuit constraints: {zkp.circuit_constraints}\")\n",
    "print(f\"Runtime: {zkp.runtime} seconds\")\n",
    "print(f\"Active input count: {zkp.active_input_count}\")\n",
    "print(f\"Leo prediction in fixed-point notation: {zkp.output[0]}\")\n",
    "print(f\"Leo prediction in decimal notation: {zkp.output_decimal[0]}\")\n",
    "print(f\"Python prediction: {python_prediction[0]}\")\n",
    "print(f\"Label: {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [ 1.711e+03  1.410e+02  1.228e+03 -3.260e+02  4.110e+02 -9.950e+02\n",
      " -1.290e+02 -1.290e+02 -1.066e+03 -1.476e+03  1.320e+02  5.360e+02\n",
      " -1.162e+03 -1.420e+03 -7.900e+01 -3.900e+01  1.083e+03  1.459e+03\n",
      "  8.000e-01  1.000e+00]\n"
     ]
    }
   ],
   "source": [
    "# print the featuers\n",
    "print(\"Features:\", test_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree\n",
    "input_array (16)['{x0: 27424i64, x1: 2240i64}', '{x0: 19664i64, x1: -5200i64}', '{x0: 6544i64, x1: -15952i64}', '{x0: -2080i64, x1: -2080i64}', '{x0: -17072i64}', '{x0: -23632i64}', '{x0: 2144i64}', '{x0: 8576i64}', '{x0: -18608i64}', '{x0: -22768i64}', '{x0: -1264i64}', '{x0: -592i64}', '{x0: 17344i64}', '{x0: 23360i64}', '{x0: 13i64}', '{x0: 16i64}']\n",
    "\n",
    "mlp\n",
    "input_array (16)['{x0: 27424i64, x1: 2240i64}', '{x0: 19664i64, x1: -5200i64}', '{x0: 6544i64, x1: -15952i64}', '{x0: -2080i64, x1: -2080i64}', '{x0: -17072i64}', '{x0: -23632i64}', '{x0: 2144i64}', '{x0: 8576i64}', '{x0: -18608i64}', '{x0: -22768i64}', '{x0: -1264i64}', '{x0: -592i64}', '{x0: 17344i64}', '{x0: 23360i64}', '{x0: 13i64}', '{x0: 16i64}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1711.0, 141.0, 1228.0, -326.0, 411.0, -995.0, -129.0, -129.0, -1066.0, -1476.0, 132.0, 536.0, -1162.0, -1420.0, -79.0, -39.0, 1083.0, 1459.0, 0.8, 1.0]\n",
      "[[ 1.78925996e+00  9.61990745e-01  9.67135916e-01 -9.42821701e-02\n",
      "   2.22980720e-01 -1.11685351e+00 -1.01618260e-01  3.67123035e-01\n",
      "  -8.91444517e-01 -1.24020654e+00  1.21819260e-01  1.46586005e-01\n",
      "  -8.15033940e-01 -8.08841700e-01  2.13862839e-02 -8.74995973e-02\n",
      "   9.98446191e-01  7.20864356e-01 -1.25764494e-03 -7.47536740e-01]]\n"
     ]
    }
   ],
   "source": [
    "features_hardcoded_in_js = [1711.0, 141.0, 1228.0, -326.0, 411.0, -995.0, -129.0, -129.0, -1066.0, -1476.0, 132.0, 536.0, -1162.0, -1420.0, -79.0, -39.0, 1083.0, 1459.0, 0.8, 1.0]\n",
    "features_hardcoded_in_js_normalized = scaler.transform([features_hardcoded_in_js])\n",
    "\n",
    "print(features_hardcoded_in_js)\n",
    "print(features_hardcoded_in_js_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit constraints: 32305\n",
      "Runtime: 6.674022197723389 seconds\n",
      "Active input count: 20\n",
      "Leo prediction in fixed-point notation: 112\n",
      "Leo prediction in decimal notation: 7.0\n",
      "Python prediction: 7\n",
      "Label: 7\n"
     ]
    }
   ],
   "source": [
    "# prove and compare the Leo prediction with the Python prediction and the label\n",
    "zkp = tree_transpiler.execute(input=features_hardcoded_in_js_normalized[0])\n",
    "python_prediction = tree.predict(features_hardcoded_in_js_normalized)\n",
    "\n",
    "print(f\"Circuit constraints: {zkp.circuit_constraints}\")\n",
    "print(f\"Runtime: {zkp.runtime} seconds\")\n",
    "print(f\"Active input count: {zkp.active_input_count}\")\n",
    "print(f\"Leo prediction in fixed-point notation: {zkp.output[0]}\")\n",
    "print(f\"Leo prediction in decimal notation: {zkp.output_decimal[0]}\")\n",
    "print(f\"Python prediction: {python_prediction[0]}\")\n",
    "print(f\"Label: {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.7893e+00,  9.6199e-01,  9.6714e-01, -9.4282e-02,  2.2298e-01,\n",
      "        -1.1169e+00, -1.0162e-01,  3.6712e-01, -8.9144e-01, -1.2402e+00,\n",
      "         1.2182e-01,  1.4659e-01, -8.1503e-01, -8.0884e-01,  2.1386e-02,\n",
      "        -8.7500e-02,  9.9845e-01,  7.2086e-01, -1.2576e-03, -7.4754e-01])\n"
     ]
    }
   ],
   "source": [
    "print(test_features_normalized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create even/odd labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_even_odd = np.ones(len(train_labels_tensor))\n",
    "train_labels_even_odd[train_labels_tensor % 2 == 0] = 0\n",
    "train_labels_even_odd_tensor = torch.tensor(train_labels_even_odd).long()\n",
    "\n",
    "validation_labels_even_odd = np.ones(len(validation_labels_tensor))\n",
    "validation_labels_even_odd[validation_labels_tensor % 2 == 0] = 0\n",
    "validation_labels_even_odd_tensor = torch.tensor(validation_labels_even_odd).long()\n",
    "\n",
    "test_labels_even_odd = np.ones(len(test_labels_tensor))\n",
    "test_labels_even_odd[test_labels_tensor % 2 == 0] = 0\n",
    "test_labels_even_odd_tensor = torch.tensor(test_labels_even_odd).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train an MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = train_features_normalized.shape[1]\n",
    "output_dim = len(set(train_labels_even_odd))  # Assuming train_labels are class indices\n",
    "hidden_dim = (input_dim + output_dim) // 2\n",
    "\n",
    "# Instantiate the model\n",
    "model_even_odd = SimpleNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 0.4227, validation loss: 0.4206\n",
      "Epoch [200], Loss: 0.3066, validation loss: 0.3048\n",
      "Epoch [300], Loss: 0.2462, validation loss: 0.2455\n",
      "Epoch [400], Loss: 0.2103, validation loss: 0.2084\n",
      "Epoch [500], Loss: 0.1893, validation loss: 0.1856\n",
      "Epoch [600], Loss: 0.1763, validation loss: 0.1710\n",
      "Epoch [700], Loss: 0.1679, validation loss: 0.1611\n",
      "Epoch [800], Loss: 0.1618, validation loss: 0.1539\n",
      "Epoch [900], Loss: 0.1571, validation loss: 0.1485\n",
      "Epoch [1000], Loss: 0.1535, validation loss: 0.1443\n",
      "Epoch [1100], Loss: 0.1507, validation loss: 0.1410\n",
      "Epoch [1200], Loss: 0.1485, validation loss: 0.1384\n",
      "Epoch [1300], Loss: 0.1465, validation loss: 0.1365\n",
      "Epoch [1400], Loss: 0.1450, validation loss: 0.1349\n",
      "Epoch [1500], Loss: 0.1435, validation loss: 0.1335\n",
      "Epoch [1600], Loss: 0.1422, validation loss: 0.1322\n",
      "Epoch [1700], Loss: 0.1412, validation loss: 0.1312\n",
      "Epoch [1800], Loss: 0.1402, validation loss: 0.1303\n",
      "Epoch [1900], Loss: 0.1394, validation loss: 0.1298\n",
      "Epoch [2000], Loss: 0.1387, validation loss: 0.1292\n",
      "Epoch [2100], Loss: 0.1379, validation loss: 0.1289\n",
      "Epoch [2200], Loss: 0.1371, validation loss: 0.1285\n",
      "Epoch [2300], Loss: 0.1361, validation loss: 0.1279\n",
      "Epoch [2400], Loss: 0.1352, validation loss: 0.1273\n",
      "Epoch [2500], Loss: 0.1342, validation loss: 0.1267\n",
      "Epoch [2600], Loss: 0.1331, validation loss: 0.1260\n",
      "Epoch [2700], Loss: 0.1323, validation loss: 0.1253\n",
      "Epoch [2800], Loss: 0.1316, validation loss: 0.1247\n",
      "Epoch [2900], Loss: 0.1309, validation loss: 0.1240\n",
      "Epoch [3000], Loss: 0.1302, validation loss: 0.1236\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_even_odd, epochs, val_loss = train(model_even_odd, train_labels_even_odd_tensor, validation_labels_even_odd_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9598"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model_even_odd, test_labels_even_odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prune the model and convert it to scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight parameters: 242\n",
      "Number of weights already zero: 0\n",
      "Number of changed weight parameters: 62\n",
      "Number of bias parameters: 13\n",
      "Number of biases already zero: 0\n",
      "Number of changed bias parameters: 2\n",
      "Percentage of weights pruned: 25.62%\n",
      "Percentage of biases pruned: 15.38%\n",
      "Remaining number of non-zero weights: 180\n",
      "Remaining number of non-zero biases: 11\n",
      "Accuracy: 0.9559\n"
     ]
    }
   ],
   "source": [
    "model_even_odd_pruned = copy.deepcopy(model_even_odd)\n",
    "model_even_odd_pruned, hooks = prune_pytorch_network(model_even_odd_pruned, 1e-1, 1e-1)\n",
    "\n",
    "_ = evaluate_model(model_even_odd_pruned, test_labels_even_odd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "converted_model_even_odd = pytorch_to_sklearn(model_even_odd_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's transpile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing number ranges and fixed-point scaling factor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Minimum number: -7.960461139678955, maximum number: 8.663869857788086. Recommended fixed-point scaling factor: 16, required Leo type: i64\n",
      "INFO:root:Transpiling model...\n",
      "INFO:root:Leo program stored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints: 84630\n",
      "Runtime for one instance: 3.0045650005340576 seconds\n",
      "Leo accuracy: 85.0 %\n",
      "Python accuracy: 85.0 %\n"
     ]
    }
   ],
   "source": [
    "# Transpile the deceision tree into Leo code\n",
    "print(type(converted_model_even_odd))\n",
    "lt = LeoTranspiler(\n",
    "    model=converted_model_even_odd, validation_data=train_features_normalized[0:600].numpy()\n",
    ")\n",
    "leo_project_path = os.path.join(os.getcwd(), \"tmp\", \"mnist\")\n",
    "leo_project_name = \"sklearn_mlp_mnist_2\"\n",
    "lt.to_leo(\n",
    "    path=leo_project_path, project_name=leo_project_name, fixed_point_scaling_factor=16\n",
    ")\n",
    "\n",
    "# Compute the accuracy of the Leo program and the Python program on the test set\n",
    "num_test_samples = len(test_features)\n",
    "\n",
    "# let's limit the number of test stamples to 10 to make the computation faster\n",
    "num_test_samples = min(num_test_samples, 20)\n",
    "\n",
    "python_predictions = np.zeros(num_test_samples)\n",
    "\n",
    "leo_predictions = np.zeros(num_test_samples)\n",
    "for i in range(num_test_samples):\n",
    "    lc = lt.run(input=test_features[i])\n",
    "    leo_predictions[i] = np.argmax(lc.output_decimal)\n",
    "    python_predictions[i] = converted_model_even_odd.predict(test_features[i].reshape(1,-1)).argmax()\n",
    "\n",
    "\n",
    "print(f\"Constraints: {lc.circuit_constraints}\")\n",
    "print(f\"Runtime for one instance: {lc.runtime} seconds\")\n",
    "\n",
    "leo_accuracy = (\n",
    "    np.sum(leo_predictions[0:num_test_samples] == test_labels_even_odd[0:num_test_samples])\n",
    "    / num_test_samples\n",
    ")\n",
    "python_accuracy = (\n",
    "    np.sum(python_predictions[0:num_test_samples] == test_labels_even_odd[0:num_test_samples])\n",
    "    / num_test_samples\n",
    ")\n",
    "\n",
    "print(f\"Leo accuracy: {100*leo_accuracy} %\")\n",
    "print(f\"Python accuracy: {100*python_accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train and transpile a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=9, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=9, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=9, random_state=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_even_odd = DecisionTreeClassifier(max_depth=9, random_state=0)\n",
    "tree_even_odd.fit(train_features_normalized, train_labels_even_odd_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for our baseline: 93.03%\n",
      "Validation accuracy for our baseline: 91.60%\n"
     ]
    }
   ],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred) * 100  # Multiply by 100 to get percentage\n",
    "\n",
    "# Evaluate the baseline model\n",
    "train_preds_baseline = tree_even_odd.predict(train_features_normalized)\n",
    "val_preds_baseline = tree_even_odd.predict(test_features_normalized)\n",
    "acc_baseline_train = acc(train_preds_baseline, train_labels_even_odd_tensor)\n",
    "acc_baseline_val = acc(val_preds_baseline, test_labels_even_odd_tensor)\n",
    "print(\n",
    "    \"Training accuracy for our baseline: \"\n",
    "    f\"{acc_baseline_train:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    \"Validation accuracy for our baseline: \"\n",
    "    f\"{acc_baseline_val:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing number ranges and fixed-point scaling factor...\n",
      "INFO:root:Minimum number: -3.603619337081909, maximum number: 5.416624069213867. Recommended fixed-point scaling factor: 16, required Leo type: i64\n",
      "INFO:root:Transpiling model...\n",
      "INFO:root:Leo program stored\n"
     ]
    }
   ],
   "source": [
    "tree_transpiler = LeoTranspiler(model=tree_even_odd, validation_data=train_features_normalized.numpy()[0:200])\n",
    "leo_project_path = os.path.join(os.getcwd(), \"tmp/mnist\")\n",
    "leo_project_name = \"tree_mnist_2\"\n",
    "tree_transpiler.to_leo(path=leo_project_path, project_name=leo_project_name, fixed_point_scaling_factor=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit constraints: 24508\n",
      "Runtime: 22.241719007492065 seconds\n",
      "Active input count: 20\n",
      "Leo prediction in fixed-point notation: 16\n",
      "Leo prediction in decimal notation: 1.0\n",
      "Python prediction: 7\n",
      "Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "# prove and compare the Leo prediction with the Python prediction and the label\n",
    "zkp = tree_transpiler.execute(input=test_features_normalized.numpy()[0])\n",
    "python_prediction = tree.predict([test_features_normalized[0]])\n",
    "\n",
    "print(f\"Circuit constraints: {zkp.circuit_constraints}\")\n",
    "print(f\"Runtime: {zkp.runtime} seconds\")\n",
    "print(f\"Active input count: {zkp.active_input_count}\")\n",
    "print(f\"Leo prediction in fixed-point notation: {zkp.output[0]}\")\n",
    "print(f\"Leo prediction in decimal notation: {zkp.output_decimal[0]}\")\n",
    "print(f\"Python prediction: {python_prediction[0]}\")\n",
    "print(f\"Label: {test_labels_even_odd[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
